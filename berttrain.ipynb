{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apsys/Downloads/startup-imperial/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf=pd.read_csv('train.csv',dtype={'text':str,'labels':int}).rename(columns={'labels':'label'}).reset_index(drop=True)\n",
    "vdf=pd.read_csv('test.csv',dtype={'text':str,'labels':int}).rename(columns={'labels':'label'}).reset_index(drop=True)\n",
    "\n",
    "tds = Dataset.from_pandas(tdf)\n",
    "vds = Dataset.from_pandas(vdf)\n",
    "\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = tds\n",
    "ds['validation'] = vds\n",
    "\n",
    "\n",
    "# labels = [label for label in ds['train'].features.keys() if label not in ['Unnamed: 0','text']]\n",
    "# id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "# label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'text', 'label'],\n",
       "    num_rows: 465\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.00ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 189.64ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 465\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import TrainingArguments,Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, Unnamed: 0. If text, Unnamed: 0 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/apsys/Downloads/startup-imperial/.venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 465\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 177\n",
      "  Number of trainable parameters = 108317962\n",
      " 33%|███▎      | 59/177 [13:05<19:54, 10.12s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, Unnamed: 0. If text, Unnamed: 0 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14\n",
      "  Batch size = 8\n",
      "                                                \n",
      " 33%|███▎      | 59/177 [13:10<19:54, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1510345935821533, 'eval_accuracy': 0.21428571428571427, 'eval_runtime': 5.0065, 'eval_samples_per_second': 2.796, 'eval_steps_per_second': 0.399, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 118/177 [27:07<10:11, 10.36s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, Unnamed: 0. If text, Unnamed: 0 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14\n",
      "  Batch size = 8\n",
      "                                                 \n",
      " 67%|██████▋   | 118/177 [27:12<10:11, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.055898427963257, 'eval_accuracy': 0.21428571428571427, 'eval_runtime': 5.2442, 'eval_samples_per_second': 2.67, 'eval_steps_per_second': 0.381, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177/177 [1:18:56<00:00,  8.88s/it]   The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, Unnamed: 0. If text, Unnamed: 0 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14\n",
      "  Batch size = 8\n",
      "                                                   \n",
      "100%|██████████| 177/177 [1:19:00<00:00,  8.88s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 177/177 [1:19:00<00:00, 26.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0245931148529053, 'eval_accuracy': 0.21428571428571427, 'eval_runtime': 3.7326, 'eval_samples_per_second': 3.751, 'eval_steps_per_second': 0.536, 'epoch': 3.0}\n",
      "{'train_runtime': 4740.1837, 'train_samples_per_second': 0.294, 'train_steps_per_second': 0.037, 'train_loss': 2.2401129943502824, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=177, training_loss=2.2401129943502824, metrics={'train_runtime': 4740.1837, 'train_samples_per_second': 0.294, 'train_steps_per_second': 0.037, 'train_loss': 2.2401129943502824, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.13.1\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    }
   ],
   "source": [
    "from transformers.onnx import FeaturesManager\n",
    "from pathlib import Path\n",
    "\n",
    "feature = \"sequence-classification\"\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "onnx_config = model_onnx_config(model.config)\n",
    "\n",
    "# export\n",
    "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
    "        preprocessor=tokenizer,\n",
    "        model=model,\n",
    "        config=onnx_config,\n",
    "        opset=13,\n",
    "        output=Path(\"trfs-model.onnx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "model = model\n",
    "tokenizer = tokenizer\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_4', 'score': 0.22812390327453613},\n",
       "  {'label': 'LABEL_0', 'score': 0.13694903254508972},\n",
       "  {'label': 'LABEL_3', 'score': 0.1114092543721199},\n",
       "  {'label': 'LABEL_6', 'score': 0.10610835999250412},\n",
       "  {'label': 'LABEL_1', 'score': 0.09675037860870361},\n",
       "  {'label': 'LABEL_2', 'score': 0.08507678657770157},\n",
       "  {'label': 'LABEL_7', 'score': 0.08245889097452164},\n",
       "  {'label': 'LABEL_8', 'score': 0.06800109148025513},\n",
       "  {'label': 'LABEL_9', 'score': 0.055534619837999344},\n",
       "  {'label': 'LABEL_5', 'score': 0.029587682336568832}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Coupons & Cashback App for online shopping We are Growing $300K MONTHLY GMV 37%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a9e8495d644770f64eff6121419e5fe8fee92ff9fcaf68deabf3ae80ea933e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
